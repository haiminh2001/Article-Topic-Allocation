{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x113496d8fb0>"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","torch.manual_seed(0)"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":31286,"status":"ok","timestamp":1637843822238,"user":{"displayName":"Hải Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYI7h2MkYpsPBPsQyrNDaGhPG2UCkk1vrAK69kqA=s64","userId":"16371132940967469863"},"user_tz":-420},"id":"7CINd0PqKhS-"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading learnt vocab...\n","Have learnt 76867 words\n"]}],"source":["#load vocab builder\n","from data_module import VocabBuilder\n","vocab_builder = VocabBuilder()\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading data...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:02<00:00,  1.04s/it]"]},{"name":"stdout","output_type":"stream","text":["Data summary:\n","train data:\n","columns: texts, labels, rows: 18927\n","num labels: \n","Sức khỏe       1511\n","Ô tô xe máy    1503\n","Giải trí       1499\n","Giáo dục       1497\n","Pháp luật      1497\n","Số hóa         1497\n","Đời sống       1492\n","Du lịch        1474\n","Thể thao       1470\n","Khoa học       1450\n","Kinh doanh     1412\n","Thế giới       1392\n","Thời sự        1233\n","Name: labels, dtype: int64\n","test data:\n","columns: texts, labels, rows: 7865\n","num labels: \n","Giải trí       614\n","Sức khỏe       613\n","Thế giới       613\n","Ô tô xe máy    609\n","Số hóa         608\n","Pháp luật      607\n","Khoa học       606\n","Kinh doanh     605\n","Giáo dục       604\n","Đời sống       602\n","Thời sự        598\n","Thể thao       596\n","Du lịch        590\n","Name: labels, dtype: int64\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from data_module import DataHolder\n","data_holder = DataHolder()\n","print(data_holder)\n","# vocab_builder.fit(data_holder.texts)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'max_vocab_length': 5, 'embedding_dim': 200, 'num_heads': 3, 'dropout': 0.1, 'lr': 0.0001, 'eps': 1e-05, 'window_size': 3, 'gpus': 1, 'hide_target_rate': 0.5}\n","Weights summary\n","==========================================\n","Dimension reduction: 0.4 M\n","Encoder: 0.5 M\n","Decoder: 1.0 M\n","==========================================\n","Total: 2.0 M\n","Actual params used for embedding: 0.5 M\n"]}],"source":["from model_module import WordEmbedder\n","#train wordembedder \n","we = WordEmbedder(load_embedder= False, max_vocab_length= 5)\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading learnt vocab...\n","Have learnt 76867 words\n","Loading data...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n"]},{"ename":"TypeError","evalue":"__init__() missing 1 required positional argument: 'labels'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20744/4045715664.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mends\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInferenceDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_index\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_splits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_holder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_builder\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mvocab_builder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_vocab_length\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text_ends\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mends\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'labels'"]}],"source":["from data_module import VocabBuilder\n","vocab_builder = VocabBuilder()\n","from data_module import DataHolder\n","data_holder = DataHolder()\n","from data_module import InferenceDataset\n","import pickle\n","ends = []\n","for i in range(10):\n","    dataset = InferenceDataset(split_index= i, labels = data_holder.train_labels, dataset_splits = 10, texts = data_holder.train_texts, vocab_builder= vocab_builder, max_vocab_length= 10, window_size= 3)\n","    ends.append(dataset.get_text_ends())\n","print(ends)\n","with open('C:\\GitHub\\Article-Topic-Allocation\\test.pickle', 'wb') as f:\n","    pickle.dump(ends, f)\n","    \n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["10\n","22220\n"]}],"source":["print(len(ends))\n","end = []\n","for i in ends:\n","    end = end + i\n","print(len(end))"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["with open('test.pickle', 'wb') as f:\n","    pickle.dump(end, f)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["22220\n"]}],"source":["with open('test.pickle', 'rb') as f:\n","    y = pickle.load(f)\n","    print(len(y))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMzZSZ6kYgdpbUOf84/5JVQ","collapsed_sections":[],"mount_file_id":"1vXJ8uWEkYjvJRw5CRnvBJ3vfm4hgIJg2","name":"Article Topic Allocation.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
